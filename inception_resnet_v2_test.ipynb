{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 220500, 1)\n",
      "(2, 1, 220500, 1)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "# Test loading files and stacking them into a batch\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "from timbre_recognition.modeling.inception_resnet_v2 import *\n",
    "from timbre_recognition.utils.module_functions import *\n",
    "from timbre_recognition.utils.io import *\n",
    "  \n",
    "y1, sr = load_wav_file_tf('timbre_recognition/datasets/ESC-50/audio/1-977-A-39.wav')\n",
    "y2, sr = load_wav_file_librosa('timbre_recognition/datasets/ESC-50/audio/1-85168-A-39.wav')\n",
    "\n",
    "print(y1.shape)\n",
    "tensor_in = tf.concat(axis=0, values=[y1, y2])\n",
    "print(tensor_in.get_shape())\n",
    "print(tensor_in.dtype.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Variables: 109\n",
      "(2, 1, 220500, 1)\n",
      "(2, 1, 55121, 32)\n",
      "(2, 1, 55102, 32)\n",
      "(2, 1, 55102, 64)\n",
      "(2, 1, 13771, 160)\n",
      "(2, 1, 3438, 192)\n",
      "(2, 1, 855, 384)\n",
      "(2, 1, 855, 384)\n",
      "(2, 1, 855, 384)\n",
      "(2, 1, 855, 384)\n",
      "(2, 1, 855, 384)\n",
      "(2, 1, 855, 384)\n",
      "(2, 1, 209, 1152)\n",
      "(2, 1, 209, 1152)\n",
      "(2, 1, 209, 1152)\n",
      "(2, 1, 209, 1152)\n",
      "(2, 1, 209, 1152)\n",
      "(2, 1, 209, 1152)\n",
      "(2, 1, 209, 1152)\n",
      "(2, 1, 209, 1152)\n",
      "(2, 1, 209, 1152)\n",
      "(2, 1, 209, 1152)\n",
      "(2, 1, 209, 1152)\n",
      "(2, 1, 48, 2144)\n",
      "(2, 1, 48, 2144)\n",
      "(2, 1, 48, 2144)\n",
      "(2, 1, 48, 2144)\n",
      "(2, 1, 1, 2144)\n",
      "(2, 128)\n",
      "(2, 128)\n"
     ]
    }
   ],
   "source": [
    "# Test single pass through network\n",
    "\n",
    "kernel_module = tf.Module()\n",
    "embedding, endpoints = inception_resnet_v2(tensor_in, kernel_module, embed_dim=128)\n",
    "#for name, cp in endpoints.items():\n",
    "#  print(name)\n",
    "#  print(' ', tf.reduce_max(cp))\n",
    "#  print(' ', tf.reduce_min(cp))\n",
    "\n",
    "print(\"Number of Variables:\", len(kernel_module.variables))\n",
    "#print_module_tree(kernel_module)\n",
    "#for v in kernel_module.variables:\n",
    "#  print(v)\n",
    "print(tensor_in.shape)\n",
    "for x in endpoints.values():\n",
    "  print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32, 48)\n",
      "[-0.02117196  0.01057281  0.01032447 -0.0133171   0.02197524  0.03374906\n",
      "  0.04677583  0.06712596  0.02091689 -0.03943025 -0.03660249  0.01657343\n",
      "  0.02763744 -0.03944011  0.0175179   0.07570648 -0.00486042  0.04881059\n",
      " -0.01953663  0.06294896]\n",
      "45326976\n"
     ]
    }
   ],
   "source": [
    "# Test accessing kernels and computing params\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "shape = kernel_module.Inception_Resnet_A1_Block.Branch_2.Conv2d_0b_20.shape\n",
    "print(shape)\n",
    "data = kernel_module.Inception_Resnet_A1_Block.Branch_2.Conv2d_0b_20.numpy()\n",
    "print(data[0,:,0,0])\n",
    "print(compute_num_params(kernel_module).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1, 220500, 1)\n",
      "(2000, 4)\n",
      "tf.Tensor([ 0 14 36 ... 25  8  0], shape=(2000,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Test loading dataset\n",
    "\n",
    "from timbre_recognition.utils.io import *\n",
    "from timbre_recognition.ops.triplet_loss import *\n",
    "from timbre_recognition.modeling.inception_resnet_v2 import *\n",
    "from timbre_recognition.modeling.inception_v4_train import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "BATCH_SIZE = 12\n",
    "EMBED_DIM = 128\n",
    "LOSS_MARGIN = 0.01\n",
    "\n",
    "data_directory = 'timbre_recognition/datasets/ESC-50/audio/'\n",
    "d, l = load_esc50_dataset(data_directory)\n",
    "print(d.shape)\n",
    "print(l.shape)\n",
    "print(tf.strings.to_number(l[:, 3], tf.dtypes.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.010543306, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Test loading dataset, batching, single pass through network, and loss\n",
    "\n",
    "from timbre_recognition.utils.io import *\n",
    "from timbre_recognition.ops.triplet_loss import *\n",
    "from timbre_recognition.modeling.inception_resnet_v2 import *\n",
    "from timbre_recognition.modeling.inception_resnet_v2_train import *\n",
    "from timbre_recognition.utils.module_functions import print_module_tree\n",
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 12\n",
    "EMBED_DIM = 128\n",
    "LOSS_MARGIN = 0.01\n",
    "BUFFER_SIZE = 2000\n",
    "\n",
    "data_directory = 'timbre_recognition/datasets/ESC-50/audio/'\n",
    "\n",
    "\"\"\"\n",
    "with tf.Graph().as_default():\n",
    "  data_placeholder = tf.placeholder(tf.float32)\n",
    "  labels_placeholder = tf.placeholder(tf.string)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((data_placeholder, labels_placeholder))\n",
    "  dataset = dataset.batch(BATCH_SIZE)\n",
    "  #iterator = dataset.make_initializable_iterator()\n",
    "  batch_data, batch_labels = next(iter(dataset)) #iterator.get_next()\n",
    "  batch_data = tf.expand_dims(batch_data, 1)\n",
    "  batch_data = tf.expand_dims(batch_data, 3)\n",
    "  embeddings, endpoints = inception_v4(batch_data, embed_dim=EMBED_DIM, reuse=tf.AUTO_REUSE)\n",
    "  triplet_loss = batch_hard_triplet_loss(batch_labels[:, 3], embeddings, LOSS_MARGIN, True) \n",
    "  \n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    data, labels = load_esc50_dataset(data_directory, sess)\n",
    "    sess.run(iterator.initializer, {data_placeholder : data,\n",
    "                                   labels_placeholder : labels})\n",
    "    print('embedding shape:',sess.run(embeddings).shape)\n",
    "    print('loss:', sess.run(triplet_loss))\n",
    "    l = sess.run(batch_labels)\n",
    "\"\"\"\n",
    "\n",
    "data, labels = load_esc50_dataset(data_directory)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "batch_data, batch_labels = next(iter(dataset))\n",
    "kernel_module = tf.Module()\n",
    "embeddings, endpoints = inception_resnet_v2(batch_data, kernel_module, embed_dim=EMBED_DIM)\n",
    "triplet_loss = batch_hard_triplet_loss(batch_labels[:, 3], embeddings, LOSS_MARGIN, True, 'mahalanobis', kernel_module)\n",
    "print(triplet_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration file...\n",
      "Configurations loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1017 19:25:34.163916 4435789248 deprecation.py:323] From /Users/reed/HonorsThesis/timbre_recognition/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tf.Tensor(0.099705815, shape=(), dtype=float32)\n",
      "Loss: tf.Tensor(5.5740185, shape=(), dtype=float32)\n",
      "Loss: tf.Tensor(0.23420058, shape=(), dtype=float32)\n",
      "Loss: tf.Tensor(0.15302522, shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-28087a0a079b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#  manager.save()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-28087a0a079b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0minception_resnet_v2_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;31m# These are here only because waiting for an epoch is too long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HonorsThesis/timbre_recognition/modeling/inception_resnet_v2_train.py\u001b[0m in \u001b[0;36minception_resnet_v2_train\u001b[0;34m(inputs, labels, kernel_module, step)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriplet_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HonorsThesis/timbre_recognition/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HonorsThesis/timbre_recognition/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/HonorsThesis/timbre_recognition/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HonorsThesis/timbre_recognition/lib/python3.7/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    594\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m           data_format=data_format),\n\u001b[0m\u001b[1;32m    597\u001b[0m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[1;32m    598\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HonorsThesis/timbre_recognition/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test running multiple epochs with checkpoints\n",
    "\n",
    "from timbre_recognition.modeling.inception_resnet_v2_train import *\n",
    "from timbre_recognition.utils.io import *\n",
    "from timbre_recognition.configs.config import *\n",
    "import tensorflow as tf\n",
    "\n",
    "config_file = 'timbre_recognition/configs/test.yaml'\n",
    "data_directory = 'timbre_recognition/datasets/ESC-50/audio/'\n",
    "\n",
    "def main():\n",
    "  print('Loading configuration file...')\n",
    "  merge_cfg_from_file(config_file)\n",
    "  print('Configurations loaded')\n",
    "  \n",
    "  data, labels = load_esc50_dataset(data_directory)\n",
    "  kernel_module = tf.Module()\n",
    "  ckpt = tf.train.Checkpoint(kernel_module=kernel_module)\n",
    "  manager = tf.train.CheckpointManager(ckpt, 'timbre_recognition/models/test/ckpts', max_to_keep=None)\n",
    "  \n",
    "  for epoch in range(cfg.TRAIN.NUM_EPOCHS):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data, labels)).shuffle(cfg.DATASET.BUFFER_SIZE).batch(cfg.TRAIN.BATCH_SIZE)\n",
    "    batch_data, batch_labels = next(iter(dataset), (None, None))\n",
    "    while batch_data is not None:\n",
    "      inception_resnet_v2_train(batch_data, batch_labels, kernel_module, epoch)\n",
    "      batch_data, batch_labels = next(iter(dataset), (None, None))\n",
    "      # These are here only because waiting for an epoch is too long\n",
    "      ckpt = tf.train.Checkpoint(kernel_module=kernel_module)\n",
    "      manager.save()\n",
    "    # This is where they would actually go\n",
    "    # ckpt = tf.train.Checkpoint(kernel_module=kernel_module)\n",
    "    #  manager.save()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['timbre_recognition/ckpts/ckpt-4', 'timbre_recognition/ckpts/ckpt-1', 'timbre_recognition/ckpts/ckpt-2', 'timbre_recognition/ckpts/ckpt-3']\n",
      "timbre_recognition/ckpts/ckpt-3\n",
      "-0.16371967\n",
      "0.16113403\n",
      "0.16136993\n"
     ]
    }
   ],
   "source": [
    "# Test restoring a model\n",
    "\n",
    "y1, sr = load_wav_file_tf('timbre_recognition/datasets/ESC-50/audio/1-977-A-39.wav')\n",
    "kernel_module = init_inception_resnet_kernels(y1.shape, cfg.MODEL.EMBED_DIM)\n",
    "ckpt = tf.train.Checkpoint(kernel_module=kernel_module)\n",
    "manager = tf.train.CheckpointManager(ckpt, 'timbre_recognition/modeling/test/ckpts', max_to_keep=None)\n",
    "print(manager.checkpoints)\n",
    "print(manager.latest_checkpoint)\n",
    "ckpt.restore(manager.checkpoints[0])\n",
    "print(kernel_module.Stem.Conv2d_1a_20.numpy()[0,0,0,0])\n",
    "ckpt.restore(manager.checkpoints[1])\n",
    "print(kernel_module.Stem.Conv2d_1a_20.numpy()[0,0,0,0])\n",
    "ckpt.restore(manager.checkpoints[2])\n",
    "print(kernel_module.Stem.Conv2d_1a_20.numpy()[0,0,0,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

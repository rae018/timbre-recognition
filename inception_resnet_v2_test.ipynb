{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 220500, 1)\n",
      "(2, 1, 220500, 1)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "# Test loading files and stacking them into a batch\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "from timbre_recognition.modeling.inception_resnet_v2 import *\n",
    "from timbre_recognition.utils.module_functions import *\n",
    "from timbre_recognition.utils.io import *\n",
    "  \n",
    "y1, sr = load_wav_file_tf('timbre_recognition/datasets/ESC-50/audio/1-977-A-39.wav')\n",
    "y2, sr = load_wav_file_librosa('timbre_recognition/datasets/ESC-50/audio/1-85168-A-39.wav')\n",
    "\n",
    "print(y1.shape)\n",
    "tensor_in = tf.concat(axis=0, values=[y1, y2])\n",
    "print(tensor_in.get_shape())\n",
    "print(tensor_in.dtype.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Variables: 109\n",
      "(2, 1, 220500, 1)\n",
      "(2, 1, 36747, 32)\n",
      "(2, 1, 36728, 32)\n",
      "(2, 1, 36728, 64)\n",
      "(2, 1, 6119, 160)\n",
      "(2, 1, 1017, 192)\n",
      "(2, 1, 167, 384)\n",
      "(2, 1, 167, 384)\n",
      "(2, 1, 167, 384)\n",
      "(2, 1, 167, 384)\n",
      "(2, 1, 167, 384)\n",
      "(2, 1, 167, 384)\n",
      "(2, 1, 74, 1152)\n",
      "(2, 1, 74, 1152)\n",
      "(2, 1, 74, 1152)\n",
      "(2, 1, 74, 1152)\n",
      "(2, 1, 74, 1152)\n",
      "(2, 1, 74, 1152)\n",
      "(2, 1, 74, 1152)\n",
      "(2, 1, 74, 1152)\n",
      "(2, 1, 74, 1152)\n",
      "(2, 1, 74, 1152)\n",
      "(2, 1, 74, 1152)\n",
      "(2, 1, 28, 2144)\n",
      "(2, 1, 28, 2144)\n",
      "(2, 1, 28, 2144)\n",
      "(2, 1, 28, 2144)\n",
      "(2, 1, 1, 2144)\n",
      "(2, 128)\n",
      "(2, 128)\n"
     ]
    }
   ],
   "source": [
    "# Test single pass through network\n",
    "\n",
    "kernel_module = tf.Module()\n",
    "embeddings, endpoints = inception_resnet_v2(tensor_in, kernel_module, embed_dim=128)\n",
    "#for name, cp in endpoints.items():\n",
    "#  print(name)\n",
    "#  print(' ', tf.reduce_max(cp))\n",
    "#  print(' ', tf.reduce_min(cp))\n",
    "\n",
    "print(\"Number of Variables:\", len(kernel_module.variables))\n",
    "#print_module_tree(kernel_module)\n",
    "#for v in kernel_module.variables:\n",
    "#print(v)\n",
    "print(tensor_in.shape)\n",
    "for x in endpoints.values():\n",
    "  print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5123, shape=(1, 1), dtype=float32, numpy=array([[-0.01500663]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.get_shape()[1]\n",
    "e1 = embeddings[0]\n",
    "e2 = embeddings[1]\n",
    "M = tf.random.uniform([128,128])\n",
    "e1 = tf.expand_dims(e1, 0)\n",
    "e2 = tf.expand_dims(e2, 0)\n",
    "tf.matmul(tf.matmul(e1-e2, M), tf.transpose(e1-e2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32, 48)\n",
      "[-0.02117196  0.01057281  0.01032447 -0.0133171   0.02197524  0.03374906\n",
      "  0.04677583  0.06712596  0.02091689 -0.03943025 -0.03660249  0.01657343\n",
      "  0.02763744 -0.03944011  0.0175179   0.07570648 -0.00486042  0.04881059\n",
      " -0.01953663  0.06294896]\n",
      "45326976\n"
     ]
    }
   ],
   "source": [
    "# Test accessing kernels and computing params\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "shape = kernel_module.Inception_Resnet_A1_Block.Branch_2.Conv2d_0b_20.shape\n",
    "print(shape)\n",
    "data = kernel_module.Inception_Resnet_A1_Block.Branch_2.Conv2d_0b_20.numpy()\n",
    "print(data[0,:,0,0])\n",
    "print(compute_num_params(kernel_module).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1, 220500, 1)\n",
      "(2000, 4)\n",
      "tf.Tensor([ 0 14 36 ... 25  8  0], shape=(2000,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Test loading dataset\n",
    "\n",
    "from timbre_recognition.utils.io import *\n",
    "from timbre_recognition.ops.triplet_loss import *\n",
    "from timbre_recognition.modeling.inception_resnet_v2 import *\n",
    "from timbre_recognition.modeling.inception_v4_train import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "BATCH_SIZE = 12\n",
    "EMBED_DIM = 128\n",
    "LOSS_MARGIN = 0.01\n",
    "\n",
    "data_directory = 'timbre_recognition/datasets/ESC-50/audio/'\n",
    "d, l = load_esc50_dataset(data_directory)\n",
    "print(d.shape)\n",
    "print(l.shape)\n",
    "print(tf.strings.to_number(l[:, 3], tf.dtypes.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.008171577, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Test loading dataset, batching, single pass through network, and loss\n",
    "\n",
    "from timbre_recognition.utils.io import *\n",
    "from timbre_recognition.ops.triplet_loss import *\n",
    "from timbre_recognition.modeling.inception_resnet_v2 import *\n",
    "from timbre_recognition.modeling.inception_resnet_v2_train import *\n",
    "from timbre_recognition.utils.module_functions import print_module_tree\n",
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 12\n",
    "EMBED_DIM = 128\n",
    "LOSS_MARGIN = 0.01\n",
    "BUFFER_SIZE = 2000\n",
    "\n",
    "data_directory = 'timbre_recognition/datasets/ESC-50/audio/'\n",
    "\n",
    "data, labels = load_esc50_dataset(data_directory)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "batch_data, batch_labels = next(iter(dataset))\n",
    "kernel_module = tf.Module()\n",
    "embeddings, endpoints = inception_resnet_v2(batch_data, kernel_module, embed_dim=EMBED_DIM)\n",
    "triplet_loss = batch_triplet_semihard_loss(batch_labels[:, 3], embeddings, LOSS_MARGIN, kernel_module)\n",
    "print(triplet_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration file...\n",
      "Configurations loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1104 00:57:55.254482 4579198400 deprecation.py:323] From /Users/reed/HonorsThesis/timbre_recognition/ops/triplet_loss.py:345: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.19843207\n",
      "Loss: 3.0237114\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-95ff900daa00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#  manager.save()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-95ff900daa00>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m       \u001b[0minception_resnet_v2_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m       \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0;31m# These are here only because waiting for an epoch is too long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HonorsThesis/timbre_recognition/modeling/inception_resnet_v2_train.py\u001b[0m in \u001b[0;36minception_resnet_v2_train\u001b[0;34m(inputs, labels, kernel_module, step)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriplet_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HonorsThesis/timbre_recognition/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HonorsThesis/timbre_recognition/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/HonorsThesis/timbre_recognition/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HonorsThesis/timbre_recognition/lib/python3.7/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    594\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m           data_format=data_format),\n\u001b[0m\u001b[1;32m    597\u001b[0m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[1;32m    598\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HonorsThesis/timbre_recognition/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test running multiple epochs with checkpoints\n",
    "\n",
    "from timbre_recognition.modeling.inception_resnet_v2_train import *\n",
    "from timbre_recognition.utils.io import *\n",
    "from timbre_recognition.configs.config import *\n",
    "import tensorflow as tf\n",
    "\n",
    "config_file = 'timbre_recognition/configs/esc-50.yaml'\n",
    "data_directory = 'timbre_recognition/datasets/ESC-50/audio/'\n",
    "\n",
    "def main():\n",
    "  print('Loading configuration file...')\n",
    "  merge_cfg_from_file(config_file)\n",
    "  print('Configurations loaded')\n",
    "  \n",
    "  data, labels = load_esc50_dataset(data_directory)\n",
    "  kernel_module = tf.Module()\n",
    "  ckpt = tf.train.Checkpoint(kernel_module=kernel_module)\n",
    "  manager = tf.train.CheckpointManager(ckpt, 'timbre_recognition/models/test/ckpts', max_to_keep=None)\n",
    "  \n",
    "  for epoch in range(cfg.TRAIN.NUM_EPOCHS):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data, labels)).shuffle(cfg.DATASET.BUFFER_SIZE).batch(cfg.TRAIN.BATCH_SIZE)\n",
    "    batch_data, batch_labels = next(iter(dataset), (None, None))\n",
    "    while batch_data is not None:\n",
    "      inception_resnet_v2_train(batch_data, batch_labels, kernel_module, epoch)\n",
    "      batch_data, batch_labels = next(iter(dataset), (None, None))\n",
    "      # These are here only because waiting for an epoch is too long\n",
    "      ckpt = tf.train.Checkpoint(kernel_module=kernel_module)\n",
    "      manager.save()\n",
    "    # This is where they would actually go\n",
    "    # ckpt = tf.train.Checkpoint(kernel_module=kernel_module)\n",
    "    #  manager.save()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 220500, 1]\n",
      "0.022859508\n"
     ]
    }
   ],
   "source": [
    "# Test restoring a model\n",
    "\n",
    "from timbre_recognition.utils.io import *\n",
    "from timbre_recognition.ops.triplet_loss import *\n",
    "from timbre_recognition.modeling.inception_resnet_v2 import *\n",
    "from timbre_recognition.configs.config import *\n",
    "\n",
    "y1, sr = load_wav_file_tf('timbre_recognition/datasets/ESC-50/audio/1-977-A-39.wav')\n",
    "s = y1.shape.as_list()\n",
    "s[0] = 1\n",
    "print(s)\n",
    "kernel_module = init_inception_resnet_kernels(s, cfg.MODEL.EMBED_DIM)\n",
    "ckpt = tf.train.Checkpoint(kernel_module=kernel_module)\n",
    "manager = tf.train.CheckpointManager(ckpt, 'timbre_recognition/models/test/ckpts', max_to_keep=None)\n",
    "print(manager.checkpoints)\n",
    "print(manager.latest_checkpoint)\n",
    "ckpt.restore(manager.checkpoints[0])\n",
    "print(kernel_module.Stem.Conv2d_1a_20.numpy()[0,0,0,0])\n",
    "ckpt.restore(manager.checkpoints[1])\n",
    "print(kernel_module.Stem.Conv2d_1a_20.numpy()[0,0,0,0])\n",
    "ckpt.restore('timbre_recognition/models/test/ckpts/ckpt-3')\n",
    "print(kernel_module.Stem.Conv2d_1a_20.numpy()[0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.5, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from timbre_recognition.utils.evaluate import *\n",
    "from timbre_recognition.ops.triplet_loss import *\n",
    "\n",
    "truth_mask = tf.random.uniform([32,32], maxval=2, dtype=tf.dtypes.int32)\n",
    "truth_mask = truth_mask < 1\n",
    "a = tf.logical_not(tf.linalg.band_part(tf.logical_not(truth_mask), -1, 0))\n",
    "b = tf.logical_not(tf.linalg.band_part(truth_mask, 0, -1))\n",
    "congruent_mask = tf.logical_and(a, b)\n",
    "\n",
    "correct = tf.math.logical_not(tf.math.logical_xor(congruent_mask, truth_mask))\n",
    "correct = tf.dtypes.cast(correct, tf.dtypes.int32)\n",
    "# Remove the diagonal since it is trivial (any sample will have distance 0 from itself)\n",
    "# The lower triangle may not be symmetric since matrix multiplication is not commutative\n",
    "correct -= tf.linalg.band_part(correct, 0, 0)\n",
    "\n",
    "accuracy = tf.reduce_sum(correct) / (2*sum(range(correct.shape[0])))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = load_esc50_test_set('timbre_recognition/datasets/ESC-50/audio/')\n",
    "labels = tf.strings.to_number(labels[:, 3], tf.dtypes.int32)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels)).shuffle(cfg.DATASET.BUFFER_SIZE).batch(cfg.TRAIN.BATCH_SIZE)\n",
    "dataset_iter = iter(dataset)\n",
    "batch_data, batch_labels = next(dataset_iter, (None, None))\n",
    "\n",
    "truth_mask = tf.convert_to_tensor([tf.math.equal(batch_labels, x) for x in batch_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(134, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "correct = tf.dtypes.cast(truth_mask, tf.dtypes.int32)\n",
    "print(tf.reduce_sum(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading dataset, batching, single pass through network, and loss\n",
    "\n",
    "from timbre_recognition.utils.io import *\n",
    "from timbre_recognition.ops.triplet_loss import *\n",
    "from timbre_recognition.modeling.inception_resnet_v2 import *\n",
    "from timbre_recognition.modeling.inception_resnet_v2_train import *\n",
    "from timbre_recognition.utils.module_functions import print_module_tree\n",
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EMBED_DIM = 128\n",
    "LOSS_MARGIN = 0.01\n",
    "BUFFER_SIZE = 2000\n",
    "NUM_CLASSES = 50\n",
    "\n",
    "data_directory = 'timbre_recognition/datasets/ESC-50/audio/'\n",
    "\n",
    "data, labels = load_esc50_dataset(data_directory)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "batch_data, batch_labels = next(iter(dataset))\n",
    "kernel_module = tf.Module()\n",
    "embeddings, endpoints = inception_resnet_v2(batch_data, kernel_module, embed_dim=EMBED_DIM)\n",
    "\n",
    "#==========================================\n",
    "\n",
    "data, labels = load_esc50_test_set(data_directory)\n",
    "class_centers = tf.random.uniform([NUM_CLASSES, EMBED_DIM], maxval=49, dtype=tf.dtypes.int32)\n",
    "labels = tf.strings.to_number(labels[:, 3], tf.dtypes.int32)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels)).batch(BATCH_SIZE)\n",
    "dataset_iter = iter(dataset)\n",
    "batch_data, batch_labels = next(dataset_iter, (None, None))\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "while batch_data is not None:\n",
    "  embeddings, endpoints = inception_resnet_v2(batch_data, kernel_module, embed_dim=EMBED_DIM)\n",
    "  distances = pairwise_mahalanobis_distances(embeddings, class_centers, kernel_module)\n",
    "  # shape [batch_size]\n",
    "  predictions = tf.argsort(distances, 1)[:,0] # 0 index holds class center closest to embedding, taken along entire batch\n",
    "  results = tf.math.equal(batch_labels, predictions)\n",
    "  num_correct = tf.math.count_nonzero(results)\n",
    "  batch_accuracy = num_correct / len(results)\n",
    "  accuracies += [batch_accuracy]\n",
    "  break\n",
    "\n",
    "  batch_data, batch_labels = next(dataset_iter, (None, None))\n",
    "accuracy = tf.reduce_mean(accuracies)\n",
    "print(\"Accuracy: {}\".accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
